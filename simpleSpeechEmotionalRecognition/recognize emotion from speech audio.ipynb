{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "endangered-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-chart",
   "metadata": {},
   "source": [
    "### Define a function extract_feature to extract the mfcc, chroma, and mel features from a sound file. This function takes 4 parameters- the file name and three Boolean parameters for the three features:\n",
    "\n",
    "    mfcc: Mel Frequency Cepstral Coefficient, represents the short-term power spectrum of a sound\n",
    "    chroma: Pertains to the 12 different pitch classes\n",
    "    mel: Mel Spectrogram Frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "veterinary-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    #can return NoneType if soundfile doesn't open\n",
    "    with soundfile.SoundFile(file_name) as f:\n",
    "        X = f.read(dtype=\"float32\")\n",
    "        sample_rate=f.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(librosa.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, mel))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-chorus",
   "metadata": {},
   "source": [
    "### lets make a dictionary of the emotions in the RAVDESS dataset, and then a list for the emotions we're going to be trying to identify from voices\n",
    "\n",
    "    RAVDESS: Ryerson Audio-Visual Database of Emotional Speech and Song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "christian-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emotions and their key from the RAVDESS set\n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "\n",
    "#emotions we are looking to identify\n",
    "observed_emotions=['calm', 'happy', 'sad', 'angry']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-saying",
   "metadata": {},
   "source": [
    "### now lets load the voice audio\n",
    "#### x, y are for features and emotions of analyzed audio. \n",
    "#### we will use glob to grab all the audio files we want. \n",
    "#### excuse the mess of a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "scientific-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test_size=0.2):\n",
    "    x,y=[],[]\n",
    "    for file in glob.glob(\"/home/ubuntu/Downloads/school/2021-FALL/dataScience/simpleSpeechEmotionalRecognition/audio/Actor_*/*.wav\"):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-foster",
   "metadata": {},
   "source": [
    "### split the dataset into training and testing sets\n",
    "#### Letâ€™s keep the test set 25% of everything and use load_data to generate x and y train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "activated-explorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 192)\n",
      "Features extracted: 180\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=load_data(test_size=0.25)\n",
    "#lets see the shape of features\n",
    "print((x_train.shape[0], x_test.shape[0]))\n",
    "#now lets get number of features extracted\n",
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-friendship",
   "metadata": {},
   "source": [
    "### This is a Multi-layer Perceptron Classifier; it optimizes the log-loss function using LBFGS or stochastic gradient descent. \n",
    "#### BFGS calculates gradient descent while preconditioning the gradient with curve information (to avoid possible local minimas). LBFGS is limitted and used when memory requirements aren't up to par with data size.\n",
    "    Unlike SVM or Naive Bayes, the MLPClassifier has an internal NN for the purpose of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "seeing-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-passing",
   "metadata": {},
   "source": [
    "### fit and train :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "engaging-tobago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate='adaptive', max_iter=500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-consciousness",
   "metadata": {},
   "source": [
    "### and now, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "periodic-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-pulse",
   "metadata": {},
   "source": [
    "### use sklearn's accuracy_score() method to test the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "crude-complaint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.17%\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "#print accuracy with 2 decimals\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-isaac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
